{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Axx43k6PqV6F"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"/gdrive/My Drive\")\n",
        "sys.path.append('..')\n",
        "sys.path.append('../..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "rxVUleRiNxNj",
        "outputId": "e517b28c-74f2-440a-8671-2ac3c980e56d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "import pandas as pd\n",
        "from keras import Input, Model, metrics\n",
        "from keras.layers import LocallyConnected2D, Concatenate, Conv2D, ZeroPadding2D\n",
        "from shapely.geometry import Point, Polygon\n",
        "\n",
        "use_optimal_permutation = True\n",
        "WIND_path = \"/gdrive/My Drive/data.pkl\"\n",
        "\n",
        "nongrid_path = \"/gdrive/My Drive/MS_winds_new.dat\"\n",
        "nongrid_permutations_path = \"/gdrive/My Drive/top10perms_GA.pkl\"\n",
        "save_results_path = \"/gdrive/My Drive\"\n",
        "\n",
        "copernicus_path = \"/gdrive/My Drive/formed_cop_shifted_data.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4ELOhx1TOyjg"
      },
      "outputs": [],
      "source": [
        "def distsqr(a, b):\n",
        "    return (a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2\n",
        "\n",
        "def prepare_sets(data, look_back, horizon, nr_predictions, train_cnt, valid_cnt, test_cnt):\n",
        "    sets, sets_sizes = [], [train_cnt, valid_cnt, test_cnt]\n",
        "    curr_s = 0\n",
        "\n",
        "    for i in range(len(sets_sizes)):\n",
        "        x_out = None\n",
        "        for j in range(look_back):\n",
        "            x_temp = np.expand_dims(data[curr_s + j:curr_s + j + sets_sizes[i]], 3)\n",
        "            x_out = x_temp if x_out is None else np.concatenate((x_out, x_temp), axis=3)\n",
        "        y_out = None\n",
        "        for j in range(nr_predictions):\n",
        "            y_out_temp = np.expand_dims(data[curr_s + j + look_back + horizon:curr_s + j + sets_sizes[i] + look_back + horizon], 3)\n",
        "            y_out = y_out_temp if y_out is None else np.concatenate((y_out, y_out_temp), axis=3)\n",
        "\n",
        "        sets.append((x_out, y_out))\n",
        "        curr_s += sets_sizes[i] + look_back + horizon\n",
        "    return sets\n",
        "\n",
        "def get_non_grid_winds_dataset(data, look_back, horizon=0, normalize=True):\n",
        "    min_val, max_val = np.min(data), np.max(data)\n",
        "\n",
        "    reshaped = []\n",
        "    for i in range(0, data.shape[0]):\n",
        "        curr = np.zeros((8, 8))\n",
        "        iter = 0\n",
        "        for j in range(8):\n",
        "            for k in range(8):\n",
        "                if iter >= 57:\n",
        "                    break\n",
        "                curr[j, k] = data[i, iter]\n",
        "                iter += 1\n",
        "        reshaped.append(curr)\n",
        "    reshaped = np.stack(reshaped, axis=0)\n",
        "\n",
        "    if normalize:\n",
        "        reshaped = (reshaped - min_val) / (max_val - min_val)\n",
        "\n",
        "    train_cnt, valid_cnt, test_cnt = 5664, 300, 361\n",
        "\n",
        "    sets = prepare_sets(reshaped, look_back, horizon, 6, train_cnt, valid_cnt, test_cnt)\n",
        "\n",
        "    return sets[0][0], sets[0][1], sets[1][0], sets[1][1], sets[2][0], sets[2][1], min_val, max_val, reshaped\n",
        "\n",
        "\n",
        "def split_sets(dicts, look_back=5, horizon = 1, normalize=True):\n",
        "    dim = int(np.sqrt(len(dicts)))\n",
        "\n",
        "    mapping = {}\n",
        "    index = 0\n",
        "\n",
        "    for i in range(dim):\n",
        "        high_el, melo = -1e9, 0\n",
        "        for key, val in dicts.items():\n",
        "            if key not in mapping and key[1] > high_el:\n",
        "                melo = key\n",
        "                high_el = key[1]\n",
        "        mapping[melo] = index\n",
        "        index += 1\n",
        "        for j in range(dim - 1):\n",
        "            k, disto = 0, 1e9\n",
        "            for key, val in dicts.items():\n",
        "                if key not in mapping and key[0] < melo[0] and distsqr(key, melo) < disto:\n",
        "                    disto = distsqr(key, melo)\n",
        "                    k = key\n",
        "            melo = k\n",
        "            mapping[k] = index\n",
        "            index += 1\n",
        "\n",
        "    for key, val in mapping.items():\n",
        "        mapping[key] = ((val % dim) * dim + int(val // dim)) % (dim * dim)\n",
        "\n",
        "    plt.figure()\n",
        "    for key, val in mapping.items():\n",
        "        y, x = int(val / dim), val % dim,\n",
        "        plt.text(key[0], key[1], str((y, x)), fontsize=12)\n",
        "        plt.plot(key[0], key[1], \".\")\n",
        "    plt.show()\n",
        "\n",
        "    data = np.zeros((25920, dim, dim))\n",
        "\n",
        "    for key, val in dicts.items():\n",
        "        id = mapping[key]\n",
        "        data[:, int(id / dim), id % dim] = copy.deepcopy(val[\"Wind\"].values[:])\n",
        "\n",
        "    min_val, max_val = data.min(), data.max()\n",
        "    if normalize:\n",
        "        data = (data - min_val) / (max_val - min_val)\n",
        "\n",
        "    train_cnt, valid_cnt, test_cnt = 15552 - look_back - horizon, 5184 - look_back - horizon, 5184 - horizon - look_back\n",
        "    sets = prepare_sets(data, look_back, horizon, 1, train_cnt, valid_cnt, test_cnt)\n",
        "\n",
        "    return sets[0][0], sets[0][1], sets[1][0], sets[1][1], sets[2][0], sets[2][1], min_val, max_val, data\n",
        "\n",
        "\n",
        "def split_sets_copernicus(data, normalize=True, horizon=0, split_prop=[0.7, 0.1, 0.2], look_back=12):\n",
        "    min_val, max_val = data.min(), data.max()\n",
        "    if normalize:\n",
        "        data = (data - min_val) / (max_val - min_val)\n",
        "\n",
        "    train_cnt, valid_cnt, test_cnt = int(data.shape[0] * split_prop[0]), int(data.shape[0] * split_prop[1]), int(data.shape[0] * split_prop[2])\n",
        "\n",
        "    sets = prepare_sets(data, look_back, horizon, 1, train_cnt - look_back - horizon, valid_cnt - look_back - horizon, test_cnt - look_back)\n",
        "    for ii in sets:\n",
        "      for jj in ii:\n",
        "        print(len(jj))\n",
        "    return sets[0][0], sets[0][1], sets[1][0], sets[1][1], sets[2][0], sets[2][1], min_val, max_val, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XWLvM185O1Un"
      },
      "outputs": [],
      "source": [
        "def get_errors(predicted, test_y,  min_val, max_val):\n",
        "    total_nodes = predicted.shape[1] * predicted.shape[2]\n",
        "\n",
        "    errs = np.zeros((predicted.shape[1], predicted.shape[2], predicted.shape[3]))\n",
        "    mape = np.zeros((predicted.shape[1], predicted.shape[2], predicted.shape[3]))\n",
        "    mae = np.zeros((predicted.shape[1], predicted.shape[2], predicted.shape[3]))\n",
        "\n",
        "    for i in range(test_y.shape[0]):\n",
        "        denorm_truth = test_y[i, :, :, :] * (max_val - min_val) + min_val\n",
        "        denorm_pred = predicted[i, :, :, :] * (max_val - min_val) + min_val\n",
        "\n",
        "        mape += np.abs((denorm_truth - denorm_pred) / denorm_truth)\n",
        "        errs += (denorm_truth - denorm_pred) ** 2\n",
        "        mae += np.abs(denorm_pred - denorm_truth)\n",
        "    errs = np.sum(errs, axis=2) / test_y.shape[0]\n",
        "\n",
        "    rmse = np.sqrt(np.sum(errs) / total_nodes)\n",
        "    mape = np.sum(np.sum(mape, axis=2) / test_y.shape[0]) / total_nodes * 100\n",
        "    mae = np.sum((np.sum(mae, axis=2) / test_y.shape[0])) / total_nodes\n",
        "\n",
        "    return rmse, mape, mae\n",
        "\n",
        "def get_errors_non_grid(predicted, test_y, min_val, max_val):\n",
        "    nr_predictions = predicted.shape[-1]\n",
        "\n",
        "    errs = np.zeros((8, 8, nr_predictions))\n",
        "    mape = np.zeros((8, 8, nr_predictions))\n",
        "    mae = np.zeros((8, 8, nr_predictions))\n",
        "\n",
        "    test_iter = 0\n",
        "    for i in range(test_y.shape[0]):\n",
        "        if i % nr_predictions != 0:\n",
        "            continue\n",
        "        test_iter += nr_predictions\n",
        "        denorm_truth = test_y[i, :, :, :] * (max_val - min_val) + min_val\n",
        "        denorm_pred = predicted[i, :, :, :] * (max_val - min_val) + min_val\n",
        "\n",
        "        mape += np.abs((denorm_truth - denorm_pred) / denorm_truth)\n",
        "        errs += (denorm_truth - denorm_pred) ** 2\n",
        "        mae += np.abs(denorm_pred - denorm_truth)\n",
        "    errs = np.sum(errs, axis=2)\n",
        "    errs /= test_iter\n",
        "    rmse = np.sqrt(np.sum(errs) / 57)\n",
        "    mape = np.sum(mape, axis=2)\n",
        "    mape /= test_iter\n",
        "    mape = np.sum(mape) / 57\n",
        "    mape *= 100\n",
        "\n",
        "    mae = np.sum(mae, axis=2)\n",
        "    mae /= test_iter\n",
        "\n",
        "    mae = np.sum(mae) / 57\n",
        "\n",
        "    return rmse, mape, mae\n",
        "########################################################################################\n",
        "def read_non_grid_data():\n",
        "    file_dataset = nongrid_path\n",
        "    with open(file_dataset) as f:\n",
        "        data = csv.reader(f, delimiter=\",\")\n",
        "        winds = []\n",
        "        for line in data:\n",
        "            winds.append((line))\n",
        "    winds = (np.array(winds)).astype(float)  # all\n",
        "    return winds\n",
        "\n",
        "def read_folder(folder_path):\n",
        "    os.chdir(folder_path)\n",
        "\n",
        "    results = {}\n",
        "    names = {}\n",
        "    for file in reversed(sorted(glob.glob(\"*\"))):\n",
        "        if not file.endswith(\".csv\"):\n",
        "            continue\n",
        "        dataframe = pd.read_csv(file, skiprows=4, header=None)\n",
        "        dataframe.columns = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Wind']\n",
        "        # read long and lat\n",
        "        iters = 0\n",
        "        with open(file, 'r') as content_file:\n",
        "            content = content_file.read().split(\"\\n\")\n",
        "\n",
        "            long, lat = float(content[1].split(',')[1]), float(content[2].split(',')[1])\n",
        "        names[(long, lat)] = file\n",
        "        results[(long, lat)] = dataframe\n",
        "    return results, names\n",
        "\n",
        "def read_gridded_pickle(debug=True, dataset_id=0, for_testsuit=False):\n",
        "    dict, names = pickle.load(open(WIND_path, \"rb\"))\n",
        "\n",
        "    # take only whats needed\n",
        "    count = 0\n",
        "    polygon = Polygon([(-85.1923, 40.4192), (-84.9561, 40.396), (-84.9909, 40.2155), (-85.2278, 40.2404)])\n",
        "\n",
        "    rev_names = {}\n",
        "    for key, val in names.items():\n",
        "        rev_names[val] = 0\n",
        "    names_in = {}\n",
        "\n",
        "    removal = []\n",
        "\n",
        "    for key, val in dict.items():\n",
        "        pnt = Point(key[0], key[1])\n",
        "\n",
        "        if polygon.contains(pnt):\n",
        "            names_in[key] = 1\n",
        "            plt.plot(key[0], key[1], '.', markersize=8, color=\"red\")\n",
        "            count += 1\n",
        "        else:\n",
        "            removal.append(key)\n",
        "            plt.plot(key[0], key[1], '.', markersize=2, color=\"grey\")\n",
        "    assert count == 100, \"Error: There should be 100 matched points!\"\n",
        "    plt.show()\n",
        "\n",
        "    mino, maxo = 1e9, -1e9\n",
        "    for key, val in dict.items():\n",
        "        if val is None:\n",
        "            continue\n",
        "        dict[key] = val[(val.Month <= 3)]\n",
        "\n",
        "        mino = min(mino, dict[key][\"Wind\"].min())\n",
        "        maxo = max(maxo, dict[key][\"Wind\"].max())\n",
        "\n",
        "    assert mino == 0.048 and maxo == 27.228, \"Error: min/max values are not as reported\"\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cfw8CWiJXDxV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PmrVdFCXO7-i"
      },
      "outputs": [],
      "source": [
        "########################################################################################################################\n",
        "class CNN():\n",
        "    def __init__(self, nr_predictions, train_x, train_y, valid_x, valid_y, test_x, test_y, layers=[(10,10)]):\n",
        "        self.input_dim, self.look_back, self.nr_predictions = (train_x[0].shape[1], train_x[0].shape[2]), train_x[0].shape[-1], nr_predictions\n",
        "        self.train_x, self.train_y, self.valid_x, self.valid_y, self.test_x, self.test_y = train_x, train_y, valid_x, valid_y, test_x, test_y\n",
        "        self.layers = layers\n",
        "\n",
        "    def get_model(self):\n",
        "        I1 = Input(shape=(self.input_dim[0], self.input_dim[1], self.look_back))\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if idx == 0:\n",
        "                Conved = Conv2D(layer[1], kernel_size=layer[0], activation='relu', padding=\"same\")(I1)\n",
        "            else:\n",
        "                Conved = Conv2D(layer[1], kernel_size=layer[0], activation='relu', padding=\"same\")(Conved)\n",
        "\n",
        "        Sing = Conv2D(self.nr_predictions, kernel_size=1, padding=\"same\")(Conved)\n",
        "        model = Model(inputs=[I1], outputs=[Sing])\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metrics.mse])\n",
        "\n",
        "        self.model = model\n",
        "        return self.model\n",
        "\n",
        "    def train(self, epochs=1):\n",
        "        print(self.train_x[0].shape)\n",
        "        print(self.train_y.shape)\n",
        "\n",
        "        hist = self.model.fit(self.train_x, self.train_y, validation_data=(self.valid_x, self.valid_y), epochs=epochs, batch_size=200, verbose=2)\n",
        "        return hist\n",
        "\n",
        "    def predict(self, data):\n",
        "        predicted = self.model.predict(data)\n",
        "        return predicted\n",
        "# ======================================================================================================================\n",
        "class LILWCNN():\n",
        "    def __init__(self, nr_predictions, train_x, train_y, valid_x, valid_y, test_x, test_y, learnable_inputs_count, local_weights_count, local_weight_receptive_field=(1,1),\n",
        "                 be_persistent=False, use_input=True, layers=(10,10)):\n",
        "        self.nr_predictions = nr_predictions\n",
        "        self.train_x, self.train_y, self.valid_x, self.valid_y, self.test_x, self.test_y = train_x, train_y, valid_x, valid_y, test_x, test_y\n",
        "\n",
        "        self.input_dim = (train_x[0].shape[1], train_x[0].shape[2])\n",
        "        self.look_back = train_x[0].shape[3]\n",
        "\n",
        "        self.learnable_inputs_count= learnable_inputs_count\n",
        "        self.local_weights_count= local_weights_count\n",
        "\n",
        "        self.local_weight_receptive_field = local_weight_receptive_field\n",
        "        self.be_persistent=be_persistent\n",
        "        self.use_input=use_input\n",
        "        self.layers = layers\n",
        "        assert learnable_inputs_count > 0 or local_weights_count > 0, \"Error: not using learnable inputs and/or local weights, use vanilla CNN instead\"\n",
        "\n",
        "    def get_model(self):\n",
        "        I1 = Input(shape=(self.input_dim[0], self.input_dim[1], self.look_back))\n",
        "        I2 = Input(shape=(self.input_dim[0], self.input_dim[1], 1))\n",
        "\n",
        "        if(self.local_weights_count > 0 and self.local_weight_receptive_field == (2,2)):\n",
        "            # We 0-pad the input, since locally connected only supports `valid` padding.\n",
        "            # Note that this only works for input shapes described in the paper\n",
        "            Padding = ZeroPadding2D(padding=((0, 1), (0, 1)))(I1)\n",
        "        else:\n",
        "            Padding = I1\n",
        "\n",
        "        K = None\n",
        "\n",
        "        if self.local_weights_count > 0:\n",
        "            C1 = LocallyConnected2D(self.local_weights_count, kernel_size=self.local_weight_receptive_field, name=\"local_weight\",\n",
        "                                input_shape=(self.input_dim[0], self.input_dim[1], self.look_back), use_bias=False)(Padding)\n",
        "            K = C1\n",
        "        if self.learnable_inputs_count > 0:\n",
        "            C2 = LocallyConnected2D(self.learnable_inputs_count, kernel_size=(1, 1), name=\"learnable_input\",\n",
        "                                input_shape=(self.input_dim[0], self.input_dim[1], 1), use_bias=False)(I2)\n",
        "            K = C2 if K is None else Concatenate(-1)([K, C2])\n",
        "\n",
        "        Merged = Concatenate(-1)([I1, K]) if self.use_input else K\n",
        "\n",
        "        Conved = Conv2D(self.layers[0][1], kernel_size=self.layers[0][0], activation='relu', padding=\"same\")(Merged)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers[1:]):\n",
        "            Concat = Concatenate(-1)([Conved, K]) if self.be_persistent else Conved\n",
        "\n",
        "            Conved = Conv2D(layer[1], kernel_size=layer[0], activation='relu', padding=\"same\")(Concat)\n",
        "\n",
        "        Sing = Conv2D(self.nr_predictions, kernel_size=1, padding=\"same\")(Conved)\n",
        "        model = Model(inputs=[I1, I2], outputs=[Sing])\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metrics.mse])\n",
        "\n",
        "        self.model = model\n",
        "        return self.model\n",
        "\n",
        "    def train(self, epochs=1):\n",
        "        hist = self.model.fit(self.train_x, self.train_y, validation_data=(self.valid_x, self.valid_y), epochs=epochs, batch_size=200, verbose=2)\n",
        "        return hist\n",
        "\n",
        "    def predict(self, data):\n",
        "        predicted = self.model.predict(data)\n",
        "        return predicted\n",
        "\n",
        "# ======================================================================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Um3BoUPhuJBU"
      },
      "outputs": [],
      "source": [
        "  def get_coords(in_arr):\n",
        "      for i in range(in_arr.shape[1]):\n",
        "          for j in range(in_arr.shape[2]):\n",
        "              in_arr[:, i, j, 0] = i / float(in_arr.shape[1] - 1)\n",
        "              in_arr[:, i, j, 1] = j / float(in_arr.shape[2] - 1)\n",
        "      return in_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmOr-EhlXTZx"
      },
      "source": [
        "# Experiment driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xbp1KZyQXGHg"
      },
      "outputs": [],
      "source": [
        "# ======================================================================================================================\n",
        "def run_experiment(task_name, predict_horizon, nr_predictions, look_back, use_optimal_permutation=False):\n",
        "    for this_horizon in predict_horizon:\n",
        "        if task_name == \"WIND\":\n",
        "            data = read_gridded_pickle(False)\n",
        "\n",
        "            train_x, train_y, valid_x, valid_y, test_x, test_y, min_val, max_val, _ = split_sets(data,\n",
        "                                                                                                 look_back=look_back,\n",
        "                                                                                                 horizon=this_horizon)\n",
        "        elif task_name == \"nongrid\":\n",
        "            data = read_non_grid_data()\n",
        "\n",
        "            train_x, train_y, valid_x, valid_y, test_x, test_y, min_val, max_val, raw_data = get_non_grid_winds_dataset(\n",
        "                data, look_back=look_back, horizon=this_horizon, normalize=True)\n",
        "\n",
        "            if use_optimal_permutation:\n",
        "                # ---------- Permute by mutual information\n",
        "\n",
        "                # load best perms\n",
        "                perms = pickle.load(open(nongrid_permutations_path, \"rb\"))[0]\n",
        "\n",
        "                train_x_perm, train_y_perm = np.zeros_like(train_x), np.zeros_like(train_y)\n",
        "                valid_x_perm, valid_y_perm = np.zeros_like(valid_x), np.zeros_like(valid_y)\n",
        "                test_x_perm, test_y_perm = np.zeros_like(test_x), np.zeros_like(test_y)\n",
        "\n",
        "                for i in range(8):\n",
        "                    for j in range(8):\n",
        "                        id = i * 8 + j\n",
        "                        if perms[id] >= 57:\n",
        "                            continue\n",
        "                        perms_y, perms_x = perms[id] // 8, perms[id] % 8\n",
        "\n",
        "                        train_x_perm[:, i, j, :] = train_x[:, perms_y, perms_x, :]\n",
        "                        train_y_perm[:, i, j, :] = train_y[:, perms_y, perms_x, :]\n",
        "\n",
        "                        valid_x_perm[:, i, j, :] = valid_x[:, perms_y, perms_x, :]\n",
        "                        valid_y_perm[:, i, j, :] = valid_y[:, perms_y, perms_x, :]\n",
        "\n",
        "                        test_x_perm[:, i, j, :] = test_x[:, perms_y, perms_x, :]\n",
        "                        test_y_perm[:, i, j, :] = test_y[:, perms_y, perms_x, :]\n",
        "\n",
        "                train_x, train_y = np.copy(train_x_perm), np.copy(train_y_perm)\n",
        "                valid_x, valid_y = np.copy(valid_x_perm), np.copy(valid_y_perm)\n",
        "                test_x, test_y = np.copy(test_x_perm), np.copy(test_y_perm)\n",
        "                # --------------------------------------------------------------------\n",
        "        elif task_name == \"copernicus\":\n",
        "            whole_data = pickle.load(open(copernicus_path, \"rb\"))\n",
        "\n",
        "            train_x, train_y, valid_x, valid_y, test_x, test_y, min_val, max_val, _ = split_sets_copernicus(\n",
        "                whole_data[\"data\"][:, :, :, 0],\n",
        "                look_back=look_back,\n",
        "                horizon=this_horizon)\n",
        "\n",
        "        input_dim = (train_x.shape[1], train_x.shape[2]) # spatial dimension\n",
        "\n",
        "        train_ones = np.ones((train_x.shape[0], input_dim[0], input_dim[1], 1))\n",
        "        valid_ones = np.ones((valid_x.shape[0], input_dim[0], input_dim[1], 1))\n",
        "        test_ones = np.ones((test_x.shape[0], input_dim[0], input_dim[1], 1))\n",
        "\n",
        "\n",
        "        train_coords, valid_coords, test_coords = np.ones((train_x.shape[0], input_dim[0], input_dim[1], 2)), np.ones((valid_x.shape[0], input_dim[0], input_dim[1], 2)), np.ones((test_x.shape[0], input_dim[0], input_dim[1], 2))\n",
        "        train_coords, valid_coords, test_coords = get_coords(train_coords), get_coords(valid_coords), get_coords(test_coords)\n",
        "\n",
        "        print(train_y.shape)\n",
        "        models = [\n",
        "            {\n",
        "                \"name\": \"Persistent LI+LW CNN\",\n",
        "                \"skip\": False,\n",
        "                \"model\": LILWCNN(nr_predictions, [train_x, train_ones], train_y, [valid_x, valid_ones], valid_y, [test_x, test_ones],\n",
        "                               test_y, 2, 2, be_persistent=True, layers=[(5, 22), (4, 22), (3, 22)])\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"LI CNN\",\n",
        "                \"skip\": False,\n",
        "                # 2 learnable inputs are used\n",
        "                \"model\": LILWCNN(nr_predictions, [train_x, train_ones], train_y, [valid_x, valid_ones], valid_y, [test_x, test_ones],\n",
        "                               test_y, 2, 0, layers=[(5, 28), (4, 30), (3, 30)])\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"CoordConv\",\n",
        "                \"skip\": False,\n",
        "                # coordinates are concatinated with the input\n",
        "                \"model\": CNN(nr_predictions, [np.concatenate((train_x, train_coords), axis=-1)], train_y,\n",
        "                             [np.concatenate((valid_x, valid_coords), axis=-1)], valid_y,\n",
        "                             [np.concatenate((test_x, test_coords), axis=-1)], test_y, layers=[(5, 28), (4, 30), (3, 30)])\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"LI+LW CNN\",\n",
        "                \"skip\": False,\n",
        "                # 2 learnable inputs and local weights are used\n",
        "                \"model\": LILWCNN(nr_predictions, [train_x, train_ones], train_y, [valid_x, valid_ones], valid_y, [test_x, test_ones],\n",
        "                               test_y, 2, 2, layers=[(5, 28), (4, 30), (3, 30)])\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        epochs_count = 100\n",
        "        extra_text = \"\"\n",
        "\n",
        "        repeat_exp_count = 1\n",
        "\n",
        "        iter = 0\n",
        "        for exp_iter in range(repeat_exp_count):\n",
        "            print(\"Starting {} / {} experiments batch\".format(exp_iter, repeat_exp_count))\n",
        "            for model in models:\n",
        "\n",
        "                if task_name == \"nongrid\" and use_optimal_permutation and \"Permuted \" not in model[\"name\"]:\n",
        "                    model[\"name\"] = \"Permuted {}\".format(model[\"name\"])\n",
        "\n",
        "                if \"skip\" in model and model[\"skip\"] is True:\n",
        "                    print(\"Skipping {} \".format(model[\"name\"]))\n",
        "                    continue\n",
        "                # initialize\n",
        "                model[\"model\"].get_model()\n",
        "                model[\"model\"].model.summary()\n",
        "                print(\"Training {} model, with {} learnable parameters\".format(model[\"name\"], model[\"model\"].model.count_params()))\n",
        "\n",
        "                train_error, valid_error, test_error = [], [], []\n",
        "                # train for epochs or something\n",
        "                for epoch in range(epochs_count):\n",
        "                    hist = model[\"model\"].train()\n",
        "                    print(\"Trained\")\n",
        "\n",
        "                    predicted = model[\"model\"].predict(model[\"model\"].test_x)\n",
        "                    if task_name == \"WIND\":\n",
        "                        test_rmse, test_mape, test_mae = get_errors(predicted, model[\"model\"].test_y, min_val, max_val)\n",
        "                    elif task_name == \"nongrid\":\n",
        "                        if use_optimal_permutation and \"Permuted\" in model[\"name\"]:  # hacky! this holds only for the permutation used in the paper\n",
        "                            predicted[:, :-2, -1, :] = 0\n",
        "                            predicted[:, 0, -2:, :] = 0\n",
        "                        else:\n",
        "                            predicted[:, 7, 1:, :] = 0\n",
        "                        #\n",
        "                        test_rmse, test_mape, test_mae = get_errors_non_grid(predicted, model[\"model\"].test_y, min_val, max_val)\n",
        "                    elif task_name == \"copernicus\":\n",
        "                        test_rmse, test_mape, test_mae = get_errors(predicted, model[\"model\"].test_y, min_val, max_val)\n",
        "\n",
        "                    print(\" Model - {} , epoch {} / {}.\".format(model[\"name\"], epoch + 1, epochs_count))\n",
        "\n",
        "                    print(\"Test RMSE - {}, MAPE - {}, MAE - {}\".format(test_rmse, test_mape, test_mae))\n",
        "                    test_error.append((test_rmse, test_mape, test_mae))\n",
        "                    train_error.append((hist.history[\"loss\"]))\n",
        "                    valid_error.append((hist.history[\"val_loss\"]))\n",
        "\n",
        "                folder = \"{}/results/real_world/{}/\".format(save_results_path, task_name)\n",
        "\n",
        "                if not os.path.isdir(folder):\n",
        "                    os.makedirs(folder)\n",
        "\n",
        "                file_name = \"{}/{}_{}_{}_{}_{}_{}\".format(folder, task_name,\n",
        "                                                                  model[\"name\"],\n",
        "                                                                  model[\"model\"].model.count_params(),\n",
        "                                                                  epochs_count,\n",
        "                                                                  str(time.ctime()).replace(\" \", \"_\").replace(\":\", \"-\"),\n",
        "                                                                  this_horizon)\n",
        "\n",
        "                pickle.dump({\"name\": model[\"name\"],\n",
        "                             \"learnable_parameters\": model[\"model\"].model.count_params(),\n",
        "                             \"nr_predictions\": nr_predictions,\n",
        "                             \"predict_horizon\": this_horizon,\n",
        "                             \"look_back\": look_back,\n",
        "                             \"summary\": model[\"model\"].model.to_json(),\n",
        "                             \"model_descriptor\": model[\"model\"].model.to_json(),\n",
        "                             \"test_error\": test_error,\n",
        "                             \"time\": time.ctime(),\n",
        "                             \"valid_error\": valid_error,\n",
        "                             \"train_error\": train_error},\n",
        "                            open(file_name, \"wb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BgPDYMrCXiLb"
      },
      "outputs": [],
      "source": [
        "  # Use id = \"WIND\" for the first experiment in the paper\n",
        "  #        = \"nongrid\" for the second experiment in the paper\n",
        "  #        = \"copernicus\" for the third experiment in the paper\n",
        "\n",
        "# run_experiment(\"nongrid\", [0], 6, 12, use_optimal_permutation=False)\n",
        "# run_experiment(\"nongrid\", [0], 6, 12, use_optimal_permutation=True)\n",
        "# run_experiment(\"copernicus\", [0, 1], 1, 8)\n",
        "run_experiment(\"WIND\", [0, 1, 2, 3, 5, 11], 1, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TLRAhPFHPVS3"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wuf4d4LuPWAU"
      },
      "outputs": [],
      "source": [
        "def get_results(task_name):\n",
        "  possible_horizons = {}\n",
        "  def read_folder(folder_path):\n",
        "      os.chdir(folder_path)\n",
        "\n",
        "      results = {}\n",
        "      for file in reversed(sorted(glob.glob(\"*\"))):\n",
        "          if os.path.isdir(file):\n",
        "              continue\n",
        "          res = pickle.load(open(file, \"rb\"))\n",
        "          key = (res[\"name\"], res[\"learnable_parameters\"], len(res[\"train_error\"]))\n",
        "\n",
        "          if key not in results:\n",
        "              results[key] = {}\n",
        "          if res[\"predict_horizon\"] not in results[key]:\n",
        "              results[key][res[\"predict_horizon\"]] = {}\n",
        "              results[key][res[\"predict_horizon\"]][\"runs\"] = []\n",
        "\n",
        "          results[key][res[\"predict_horizon\"]][\"runs\"].append(res)\n",
        "      return results\n",
        "\n",
        "  folder = \"{}/results/real_world/{}/\".format(save_results_path, task_name)\n",
        "\n",
        "  results = read_folder(folder)\n",
        "  for key, vals in results.items():\n",
        "      for horizon_key, val in vals.items():\n",
        "          valid_errs, test_mse_errs, learnable_params = None, None, []\n",
        "\n",
        "          for run in val[\"runs\"]:\n",
        "              use_epochs = len(run[\"test_error\"])\n",
        "\n",
        "              possible_horizons[run[\"predict_horizon\"]] = 1\n",
        "\n",
        "              learnable_params.append(run[\"learnable_parameters\"])\n",
        "              valid_errs = np.array(run[\"valid_error\"][:use_epochs]) if valid_errs is None else np.hstack((valid_errs,np.array(run[\"valid_error\"][:use_epochs])))\n",
        "              test_errs = np.array(run[\"test_error\"][:use_epochs])[:,0][None].T if test_mse_errs is None else np.hstack((test_mse_errs,np.array(run[\"test_error\"][:use_epochs])[:,0][None].T))\n",
        "\n",
        "          learnable_params = np.mean(learnable_params).astype(int)\n",
        "          def get_batch_stats(batch):\n",
        "              mean_batch = np.mean(batch, axis=-1)\n",
        "\n",
        "              min_batch = np.min(batch, axis=-1)\n",
        "              max_batch = np.max(batch,axis=-1)\n",
        "              std_batch = np.std(batch, axis=-1)\n",
        "\n",
        "              return std_batch, mean_batch, min_batch, max_batch\n",
        "\n",
        "          std_test, mean_test, min_test, max_test = get_batch_stats(test_errs)\n",
        "          std_valid, mean_valid, min_valid, max_valid = get_batch_stats(valid_errs)\n",
        "\n",
        "          best_valid_id = np.argmin(mean_valid)\n",
        "\n",
        "          valid_errs = np.array(valid_errs)\n",
        "          if len(valid_errs.shape) == 1:\n",
        "              valid_errs = np.expand_dims(valid_errs, axis=-1)\n",
        "          best_valid_individ_ids = np.atleast_2d(np.argmin(valid_errs, axis=0))\n",
        "\n",
        "          indexes = []\n",
        "\n",
        "          cor_valids, cor_tests = [], []\n",
        "\n",
        "          for col, index in enumerate(best_valid_individ_ids.flatten()):\n",
        "              cor_tests.append(test_errs[index, col])\n",
        "              cor_valids.append(valid_errs[index,col])\n",
        "\n",
        "          cor_tests = np.array(cor_tests)\n",
        "          best_test_individ_mean = np.mean(cor_tests)\n",
        "          best_test_individ_std = np.std(cor_tests)\n",
        "\n",
        "          best_valid_mean = np.mean(cor_valids)\n",
        "          results[key][horizon_key][\"formed_sting\"] = \" | {:<15} | {:<15}  \".format(round(best_valid_mean,4), round(best_test_individ_mean, 4))\n",
        "  # form table\n",
        "  names = sorted(list(results.keys()), key=lambda x: x[0].replace(\"Permuted \", \"\"))\n",
        "  for key in names:\n",
        "      vals = results[key]\n",
        "      horizons_list = sorted(list(possible_horizons.keys()))\n",
        "      lines = [ \"{:>40} {:<10}\".format(key[0], str(key[1]) )]\n",
        "\n",
        "      for horizon in horizons_list:\n",
        "          if horizon not in vals:\n",
        "              lines.append(\"- & -\")\n",
        "              continue\n",
        "          lines.append(vals[horizon][\"formed_sting\"])\n",
        "      \n",
        "      total_len = 0\n",
        "      for index, j in enumerate(lines):\n",
        "          spc = \"  \"\n",
        "          if index == 0:\n",
        "              spc = \"   \"\n",
        "          print(spc + j , end=\"\",flush=True)\n",
        "          total_len += len(j)\n",
        "      print()\n",
        "      print(\"-\"*total_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6PRtDEPLiJoz"
      },
      "outputs": [],
      "source": [
        "# get_results(\"copernicus\")\n",
        "# get_results(\"WIND\")\n",
        "# get_results(\"nongrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UIkdd6-aiM8K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Localized_CNNs_on_real_world_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
